import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import (
    accuracy_score, 
    precision_recall_fscore_support, 
    classification_report,
    confusion_matrix
)
from sklearn.model_selection import train_test_split
from gemini_model import GeminiIntentClassifier, load_and_prepare_data
import os
from typing import Dict, List, Tuple
import time

class ModelEvaluator:
    """
    Model performansÄ±nÄ± deÄŸerlendiren ve gÃ¶rselleÅŸtiren sÄ±nÄ±f
    """
    
    def __init__(self):
        self.metrics_history = []
        
    def calculate_metrics(self, y_true: List[str], y_pred: List[str]) -> Dict:
        """
        Temel performans metriklerini hesaplar
        
        Args:
            y_true (List[str]): GerÃ§ek etiketler
            y_pred (List[str]): Tahmin edilen etiketler
            
        Returns:
            Dict: Performans metrikleri
        """
        # Temel metrikler
        accuracy = accuracy_score(y_true, y_pred)
        precision, recall, f1, support = precision_recall_fscore_support(
            y_true, y_pred, average='weighted'
        )
        
        # SÄ±nÄ±f bazlÄ± metrikler
        precision_per_class, recall_per_class, f1_per_class, support_per_class = precision_recall_fscore_support(
            y_true, y_pred, average=None
        )
        
        # DetaylÄ± rapor
        report = classification_report(y_true, y_pred, output_dict=True)
        
        # Confusion matrix
        cm = confusion_matrix(y_true, y_pred)
        
        # Benzersiz sÄ±nÄ±flar
        classes = sorted(list(set(y_true + y_pred)))
        
        metrics = {
            'accuracy': accuracy,
            'precision_weighted': precision,
            'recall_weighted': recall,
            'f1_weighted': f1,
            'precision_per_class': dict(zip(classes, precision_per_class)),
            'recall_per_class': dict(zip(classes, recall_per_class)),
            'f1_per_class': dict(zip(classes, f1_per_class)),
            'support_per_class': dict(zip(classes, support_per_class)),
            'confusion_matrix': cm,
            'classes': classes,
            'detailed_report': report
        }
        
        return metrics
    
    def print_metrics_table(self, metrics: Dict):
        """
        Metrikleri tablo formatÄ±nda yazdÄ±rÄ±r
        
        Args:
            metrics (Dict): Hesaplanan metrikler
        """
        print("\n" + "="*80)
        print("ðŸŽ¯ MODEL PERFORMANS RAPORU")
        print("="*80)
        
        # Genel metrikler
        print(f"\nðŸ“Š GENEL METRIKLER:")
        print(f"   â€¢ Accuracy (DoÄŸruluk):     {metrics['accuracy']:.4f} ({metrics['accuracy']*100:.2f}%)")
        print(f"   â€¢ Precision (Kesinlik):    {metrics['precision_weighted']:.4f}")
        print(f"   â€¢ Recall (DuyarlÄ±lÄ±k):     {metrics['recall_weighted']:.4f}")
        print(f"   â€¢ F1-Score:                {metrics['f1_weighted']:.4f}")
        
        # SÄ±nÄ±f bazlÄ± metrikler
        print(f"\nðŸ“‹ SINIF BAZLI DETAYLAR:")
        print(f"{'Intent':<20} {'Precision':<10} {'Recall':<10} {'F1-Score':<10} {'Support':<10}")
        print("-" * 70)
        
        for intent in metrics['classes']:
            precision = metrics['precision_per_class'][intent]
            recall = metrics['recall_per_class'][intent]
            f1 = metrics['f1_per_class'][intent]
            support = metrics['support_per_class'][intent]
            
            print(f"{intent:<20} {precision:<10.4f} {recall:<10.4f} {f1:<10.4f} {support:<10}")
        
        print("-" * 70)
        
        # En iyi ve en kÃ¶tÃ¼ performans gÃ¶steren sÄ±nÄ±flar
        f1_scores = metrics['f1_per_class']
        best_class = max(f1_scores, key=f1_scores.get)
        worst_class = min(f1_scores, key=f1_scores.get)
        
        print(f"\nðŸ† En Ä°yi Performans:  {best_class} (F1: {f1_scores[best_class]:.4f})")
        print(f"âš ï¸  En ZayÄ±f Performans: {worst_class} (F1: {f1_scores[worst_class]:.4f})")
    
    def plot_confusion_matrix(self, metrics: Dict, save_path: str = None):
        """
        Confusion matrix'i gÃ¶rselleÅŸtirir
        
        Args:
            metrics (Dict): Hesaplanan metrikler
            save_path (str): GÃ¶rselin kaydedileceÄŸi yol
        """
        plt.figure(figsize=(10, 8))
        cm = metrics['confusion_matrix']
        classes = metrics['classes']
        
        # Normalize edilmiÅŸ confusion matrix
        cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        
        sns.heatmap(
            cm_normalized,
            annot=True,
            fmt='.2f',
            cmap='Blues',
            xticklabels=classes,
            yticklabels=classes,
            cbar_kws={'label': 'Normalize EdilmiÅŸ DeÄŸerler'}
        )
        
        plt.title('Confusion Matrix (KarÄ±ÅŸÄ±klÄ±k Matrisi)', fontsize=16, fontweight='bold')
        plt.xlabel('Tahmin Edilen SÄ±nÄ±f', fontsize=12)
        plt.ylabel('GerÃ§ek SÄ±nÄ±f', fontsize=12)
        plt.xticks(rotation=45, ha='right')
        plt.yticks(rotation=0)
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"ðŸ“Š Confusion matrix ÅŸuraya kaydedildi: {save_path}")
        
        plt.show()
    
    def plot_performance_metrics(self, metrics: Dict, save_path: str = None):
        """
        Performans metriklerini bar chart olarak gÃ¶rselleÅŸtirir
        
        Args:
            metrics (Dict): Hesaplanan metrikler
            save_path (str): GÃ¶rselin kaydedileceÄŸi yol
        """
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
        
        # Sol grafik: SÄ±nÄ±f bazlÄ± F1 skorlarÄ±
        classes = metrics['classes']
        f1_scores = [metrics['f1_per_class'][cls] for cls in classes]
        
        bars1 = ax1.bar(classes, f1_scores, color='skyblue', alpha=0.8)
        ax1.set_title('SÄ±nÄ±f BazlÄ± F1 SkorlarÄ±', fontsize=14, fontweight='bold')
        ax1.set_xlabel('Intent SÄ±nÄ±flarÄ±')
        ax1.set_ylabel('F1 Skoru')
        ax1.tick_params(axis='x', rotation=45)
        ax1.set_ylim(0, 1)
        
        # DeÄŸerleri bar'larÄ±n Ã¼zerine yaz
        for bar, score in zip(bars1, f1_scores):
            ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                    f'{score:.3f}', ha='center', va='bottom', fontsize=10)
        
        # SaÄŸ grafik: Genel metrikler
        general_metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']
        values = [
            metrics['accuracy'],
            metrics['precision_weighted'],
            metrics['recall_weighted'],
            metrics['f1_weighted']
        ]
        
        bars2 = ax2.bar(general_metrics, values, color=['gold', 'lightcoral', 'lightgreen', 'plum'])
        ax2.set_title('Genel Performans Metrikleri', fontsize=14, fontweight='bold')
        ax2.set_ylabel('Skor')
        ax2.set_ylim(0, 1)
        
        # DeÄŸerleri bar'larÄ±n Ã¼zerine yaz
        for bar, value in zip(bars2, values):
            ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                    f'{value:.3f}', ha='center', va='bottom', fontsize=10)
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"ðŸ“Š Performans grafikleri ÅŸuraya kaydedildi: {save_path}")
        
        plt.show()
    
    def create_sample_results_table(self, test_texts: List[str], y_true: List[str], 
                                  y_pred: List[str], n_samples: int = 20) -> pd.DataFrame:
        """
        Ã–rnek sonuÃ§lar tablosu oluÅŸturur
        
        Args:
            test_texts (List[str]): Test metinleri
            y_true (List[str]): GerÃ§ek etiketler
            y_pred (List[str]): Tahmin edilen etiketler
            n_samples (int): GÃ¶sterilecek Ã¶rnek sayÄ±sÄ±
            
        Returns:
            pd.DataFrame: Ã–rnek sonuÃ§lar tablosu
        """
        # Rastgele Ã¶rnekler seÃ§
        indices = np.random.choice(len(test_texts), min(n_samples, len(test_texts)), replace=False)
        
        results = []
        for i in indices:
            correct = "âœ…" if y_true[i] == y_pred[i] else "âŒ"
            results.append({
                'Metin': test_texts[i][:50] + "..." if len(test_texts[i]) > 50 else test_texts[i],
                'GerÃ§ek Intent': y_true[i],
                'Tahmin Edilen': y_pred[i],
                'DoÄŸru': correct
            })
        
        return pd.DataFrame(results)

def run_full_evaluation():
    """
    Tam model deÄŸerlendirmesi yapar
    """
    print("ðŸš€ Gemini Motivasyon Chatbot - Tam DeÄŸerlendirme BaÅŸlÄ±yor...")
    
    # Veri setini yÃ¼kle
    data_path = 'data/chatbot_dataset.csv'
    X_train, X_test, y_train, y_test = load_and_prepare_data(data_path)
    
    print(f"ðŸ“Š Test seti boyutu: {len(X_test)}")
    
    # Evaluator oluÅŸtur
    evaluator = ModelEvaluator()
    
    # Gemini classifier oluÅŸtur
    classifier = GeminiIntentClassifier()
    
    # API anahtarÄ± kontrolÃ¼
    if not os.getenv('GOOGLE_API_KEY'):
        print("âš ï¸  GOOGLE_API_KEY Ã§evre deÄŸiÅŸkeni ayarlanmamÄ±ÅŸ!")
        print("ðŸ“ SimÃ¼le edilmiÅŸ sonuÃ§lar gÃ¶steriliyor...")
        
        # SimÃ¼le edilmiÅŸ sonuÃ§lar (Demo amaÃ§lÄ±)
        y_pred_simulated = create_simulated_predictions(y_test)
        
        # Metrikleri hesapla
        metrics = evaluator.calculate_metrics(y_test, y_pred_simulated)
        
        # SonuÃ§larÄ± gÃ¶ster
        evaluator.print_metrics_table(metrics)
        
        # GÃ¶rselleÅŸtirmeler
        evaluator.plot_confusion_matrix(metrics, 'confusion_matrix.png')
        evaluator.plot_performance_metrics(metrics, 'performance_metrics.png')
        
        # Ã–rnek sonuÃ§lar tablosu
        sample_results = evaluator.create_sample_results_table(X_test, y_test, y_pred_simulated)
        print("\nðŸ“‹ Ã–RNEK SONUÃ‡LAR:")
        print(sample_results.to_string(index=False))
        
    else:
        print("ðŸ”‘ API anahtarÄ± bulundu, gerÃ§ek deÄŸerlendirme yapÄ±lÄ±yor...")
        # GerÃ§ek API Ã§aÄŸrÄ±larÄ± yapÄ±labilir
        # Bu kÄ±sÄ±m API anahtarÄ± olduÄŸunda aktif hale gelir
        pass

def create_simulated_predictions(y_true: List[str]) -> List[str]:
    """
    Demo amaÃ§lÄ± simÃ¼le edilmiÅŸ tahminler oluÅŸturur
    
    Args:
        y_true (List[str]): GerÃ§ek etiketler
        
    Returns:
        List[str]: SimÃ¼le edilmiÅŸ tahminler
    """
    np.random.seed(42)  # Tekrarlanabilirlik iÃ§in
    
    # %85 doÄŸruluk oranÄ± ile simÃ¼lasyon
    accuracy_target = 0.85
    
    predictions = []
    all_intents = list(set(y_true))
    
    for true_label in y_true:
        if np.random.random() < accuracy_target:
            # DoÄŸru tahmin
            predictions.append(true_label)
        else:
            # YanlÄ±ÅŸ tahmin - rastgele baÅŸka bir intent seÃ§
            wrong_intents = [intent for intent in all_intents if intent != true_label]
            predictions.append(np.random.choice(wrong_intents))
    
    return predictions

if __name__ == "__main__":
    # Tam deÄŸerlendirme Ã§alÄ±ÅŸtÄ±r
    run_full_evaluation() 